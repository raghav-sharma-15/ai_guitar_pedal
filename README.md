# AI Guitar Pedal

A Python-based, real-time guitar effects pedal driven by machine-learning genre classification.  
It supports two pipelines:

1. **MLP** on hand-crafted audio features  
2. **CNN** on log-mel spectrograms  

Plug your guitar into any USB audio interface and let the pedal auto-detect genre and apply custom FX chains liveâ€”or run it offline on WAV files.

---

## ğŸš€ Quickstart

These commands assume youâ€™re in the project root (`AI_GUITAR_PEDAL/`) and have Python 3.9+ installed.

1. **Clone & set up environment**  
   ```bash
   git clone https://github.com/<your-username>/AI_GUITAR_PEDAL.git
   cd AI_GUITAR_PEDAL
   python3 -m venv venv
   source venv/bin/activate
   pip install --upgrade pip
   pip install numpy scipy librosa soundfile torch scikit-learn joblib sounddevice
Download GTZAN dataset
Go to Kaggle GTZAN genre dataset
Unzip into data/gtzan, so you have e.g. data/gtzan/blues/blues.00000.wav.
Preprocess audio
Hand-crafted features (for the MLP):
python scripts/preprocess_audio.py features
# â†’ data/features.npy
Log-mel spectrograms (for the CNN):
python scripts/preprocess_audio.py spectrogram
# â†’ data/specs.npz
Train your models
MLP on features:
python scripts/train_model.py --model-type mlp --data data/features.npy
# â†’ models/mlp_best.pth, models/mlp_last.pth, models/scaler.pkl
CNN on spectrograms:
python scripts/train_model.py --model-type cnn --data data/specs.npz
# â†’ models/cnn_best.pth, models/cnn_last.pth
Offline inference
MLP:
python main.py \
  --input data/gtzan/blues/blues.00000.wav \
  --output out_mlp.wav \
  --model models/mlp_best.pth \
  --scaler models/scaler.pkl
CNN:
python main_cnn.py \
  --input data/gtzan/blues/blues.00000.wav \
  --output out_cnn.wav \
  --model models/cnn_best.pth
Run live
python live_guitar_pedal.py
(Use --device "<Your Audio Device Name>" if you have multiple interfaces.)
ğŸ—‚ Directory Structure

AI_GUITAR_PEDAL/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ gtzan/â€¦                # raw GTZAN WAVs
â”‚   â””â”€â”€ features.npy           # auto-generated by preprocess_audio.py
â”‚   â””â”€â”€ specs.npz              # auto-generated by preprocess_audio.py
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ mlp_best.pth           # best MLP checkpoint
â”‚   â””â”€â”€ mlp_last.pth           # last MLP checkpoint
â”‚   â””â”€â”€ cnn_best.pth           # best CNN checkpoint
â”‚   â””â”€â”€ cnn_last.pth           # last CNN checkpoint
â”‚   â””â”€â”€ scaler.pkl             # feature scaler for MLP
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ effects.py             # genre-specific FX chains
â”‚   â”œâ”€â”€ preprocess_audio.py    # feature & spectrogram extraction
â”‚   â””â”€â”€ train_model.py         # MLP & CNN training routines
â”‚
â”œâ”€â”€ main.py                    # offline MLP inference + FX
â”œâ”€â”€ main_cnn.py                # offline CNN inference + FX
â”œâ”€â”€ live_guitar_pedal.py       # real-time audio stream + FX
â””â”€â”€ README.md                  # this file
ğŸ¤ Contributing

Fork the repo
Create a branch (git checkout -b feat/YourFeature)
Commit your changes (git commit -m "Add feature")
Push (git push origin feat/YourFeature)
Open a Pull Request
Please follow existing code style and update documentation as needed.

ğŸ“ License

This project is MIT-licensed. See LICENSE for details.
